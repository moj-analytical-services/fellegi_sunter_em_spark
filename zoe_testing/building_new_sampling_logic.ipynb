{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and linker set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splink.duckdb.linker import DuckDBLinker\n",
    "import pandas as pd\n",
    "from splink.splink_dataframe import SplinkDataFrame\n",
    "from typing import TYPE_CHECKING\n",
    "# https://stackoverflow.com/questions/39740632/python-type-hinting-without-cyclic-imports\n",
    "if TYPE_CHECKING:\n",
    "    from .linker import Linker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up linker\n",
    "\n",
    "# Simple dummy df\n",
    "person_ids = [i + 1 for i in range(5)]\n",
    "df = pd.DataFrame({\"person_id\": person_ids})\n",
    "\n",
    "settings = {\n",
    "    \"link_type\": \"dedupe_only\",\n",
    "    \"unique_id_column_name\": \"person_id\",\n",
    "    \"retain_intermediate_calculation_columns\": True,\n",
    "}\n",
    "linker = DuckDBLinker(df, settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trialing on simple dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up edges, clusters and cluster metrics tables\n",
    "\n",
    "# Dummy edges df\n",
    "person_ids_l = [1, 1, 4, 10, 12]\n",
    "person_ids_r = [2, 3, 5, 11, 13]\n",
    "match_probabilities = [0.99, 0.99, 0.99, 0.99, 0.95]\n",
    "\n",
    "edges_data = {\n",
    "    \"match_probability\": match_probabilities,\n",
    "    \"person_id_l\": person_ids_l,\n",
    "    \"person_id_r\": person_ids_r,\n",
    "}\n",
    "edges = pd.DataFrame(edges_data)\n",
    "\n",
    "# Dummy clusters df\n",
    "cluster_ids = [\"A\", \"A\", \"A\", \"B\", \"B\"]\n",
    "clusters_data = {\"cluster_id\": cluster_ids, \"person_id\": person_ids}\n",
    "clusters = pd.DataFrame(clusters_data)\n",
    "\n",
    "# Dummy cluster metrics table\n",
    "cluster = [\"A\", \"B\", \"C\", \"D\", \"E\",\"F\", \"G\", \"H\", \"I\"]\n",
    "n_nodes = [2, 3, 3, 10, 10, 10, 20, 20, 20]\n",
    "n_edges = [2, 2, 2, 9, 25, 36, 22, 29, 45]\n",
    "density = [\n",
    "    (n_edges * 2) / (n_nodes * (n_nodes - 1))\n",
    "    for n_nodes, n_edges in zip(n_nodes, n_edges)\n",
    "]\n",
    "df_metrics = pd.DataFrame(\n",
    "    {\"cluster_id\": cluster, \"n_nodes\": n_nodes, \"n_edges\": n_edges, \"density\": density}\n",
    ")\n",
    "df_metrics\n",
    "\n",
    "# Create splink dataframes from tables\n",
    "df_cluster_metrics = linker.register_table(\n",
    "    df_metrics, \"df_cluster_metrics\", overwrite=True\n",
    ")\n",
    "df_predict = linker.register_table(edges, \"df_predict\", overwrite=True)\n",
    "df_clustered = linker.register_table(clusters, \"df_clustered\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified sampling by density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unique id persists throughout the linking process\n",
    "\n",
    "User journey:\n",
    "Have run clusters and metrics and generate the cluster dashboard (sample by density)\n",
    "Delete it and come back to remake the next day - want the same clusters to appear (assuming data hasn't changed here)\n",
    "It might not be exactly the same clusters if random sampling happens but will be the same population and in the same order as the data hasn't changed.\n",
    "\n",
    "User journey:\n",
    "Do some cluster QA, make changes, rerun clustering and remake dashboard (now the clusters data has possibly changed)\n",
    "The cluster labels remain the same as long as the cluster membership hasn't changed?\n",
    "The ordering of clusters may well have changed though.\n",
    "It would be nice to ensure that the same lowest density clusters are always chosen where possible to see the impact of the changes the user is making.\n",
    "How could I ensure this? I could order the clusters within the partition according to density AND cluster id?\n",
    "Alternatively, could used the connected components table, order asw in by_size and join on the density table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_lowest_density_clusters(\n",
    "    linker: \"Linker\",\n",
    "    df_cluster_metrics: SplinkDataFrame,\n",
    "    rows_per_partition: int,\n",
    "    min_nodes: int,\n",
    "):\n",
    "    \"\"\"Returns ids of lowest density clusters of different sizes by\n",
    "    performing stratified sampling.\n",
    "\n",
    "    Args:\n",
    "        linker: An instance of the Splink Linker class.\n",
    "        df_cluster_metrics (SplinkDataFrame): dataframe containing\n",
    "        cluster metrics including density.\n",
    "        rows_per_partition (int): number of rows in each strata (partition)\n",
    "        min_nodes (int): minimum number of nodes a cluster must contain\n",
    "        to be included in the sample.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of cluster ids of lowest density clusters of different sizes.\n",
    "    \"\"\"\n",
    "\n",
    "    sql = f\"\"\"\n",
    "    select\n",
    "        cluster_id,\n",
    "        n_nodes,\n",
    "        density,\n",
    "        row_number() over (partition by n_nodes order by density, cluster_id desc) as row_num\n",
    "    from {df_cluster_metrics.physical_name}\n",
    "    where n_nodes >= {min_nodes}\n",
    "    \"\"\"\n",
    "\n",
    "    linker._enqueue_sql(sql, \"__splink__partition_clusters_by_size\")\n",
    "\n",
    "    sql = f\"\"\"\n",
    "    select\n",
    "        cluster_id,\n",
    "        round(density, 4) as density_4dp,\n",
    "        n_nodes\n",
    "    from __splink__partition_clusters_by_size\n",
    "    where row_num <= {rows_per_partition}\n",
    "    order by n_nodes\n",
    "    \"\"\"\n",
    "\n",
    "    linker._enqueue_sql(sql, \"__splink__lowest_density_clusters\")\n",
    "    df_lowest_density_clusters = linker._execute_sql_pipeline()\n",
    "\n",
    "    return df_lowest_density_clusters.as_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = _get_lowest_density_clusters(linker, df_cluster_metrics, 1, 3)\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ids = [r['cluster_id'] for r in test]\n",
    "cluster_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_metrics.as_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dashboard - doesn't work on dummy data because not set up accurately?\n",
    "\n",
    "linker.cluster_studio_dashboard(\n",
    "    df_predict,\n",
    "    df_clustered,\n",
    "    out_path=\"cluster_studio.html\",\n",
    "    sampling_method=\"lowest_density_clusters_by_size\",\n",
    "    overwrite=True,\n",
    "    _df_cluster_metrics=df_cluster_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dashboard - doesn't work on dummy data because not set up accurately?\n",
    "\n",
    "linker.cluster_studio_dashboard(\n",
    "    df_predict,\n",
    "    df_clustered,\n",
    "    out_path=\"cluster_studio.html\",\n",
    "    sampling_method=\"by_cluster_size\",\n",
    "    overwrite=True,\n",
    "    _df_cluster_metrics=df_cluster_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test exception - working\n",
    "linker.cluster_studio_dashboard(\n",
    "    df_predict,\n",
    "    df_clustered,\n",
    "    out_path=\"cluster_studio.html\",\n",
    "    sampling_method=\"lowest_density_clusters_by_size\",\n",
    "    sample_size=10,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on realistic dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/zoe.slade/coding_projects/splink_folder/splink\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: Jinja2>=3.0.3 in /Users/zoe.slade/.local/share/virtualenvs/splink-bxsLLt4m/lib/python3.9/site-packages (from splink==3.9.10) (3.1.2)\n",
      "Requirement already satisfied: altair<6.0.0,>=5.0.1 in /Users/zoe.slade/.local/share/virtualenvs/splink-bxsLLt4m/lib/python3.9/site-packages (from splink==3.9.10) (5.1.2)\n",
      "Requirement already satisfied: duckdb>=0.8.0 in /Users/zoe.slade/.local/share/virtualenvs/splink-bxsLLt4m/lib/python3.9/site-packages (from splink==3.9.10) (0.9.0)\n",
      "Requirement already satisfied: jsonschema<5.0,>=3.2 in /Users/zoe.slade/.local/share/virtualenvs/splink-bxsLLt4m/lib/python3.9/site-packages (from splink==3.9.10) (4.17.3)\n",
      "Requirement already satisfied: pandas>1.3.0 in /Users/zoe.slade/.local/share/virtualenvs/splink-bxsLLt4m/lib/python3.9/site-packages (from splink==3.9.10) (2.1.1)\n",
      "Requirement already satisfied: phonetics<2.0.0,>=1.0.5 in /Users/zoe.slade/.local/share/virtualenvs/splink-bxsLLt4m/lib/python3.9/site-packages (from splink==3.9.10) (1.0.5)\n",
      "Requirement already satisfied: sqlglot<19.0.0,>=13.0.0 in /Users/zoe.slade/.local/share/virtualenvs/splink-bxsLLt4m/lib/python3.9/site-packages (from splink==3.9.10) (18.17.0)\n",
      "Requirement already satisfied: numpy in /Users/zoe.slade/.local/share/virtualenvs/splink-bxsLLt4m/lib/python3.9/site-packages (from altair<6.0.0,>=5.0.1->splink==3.9.10) (1.26.0)\n",
      "Requirement already satisfied: packaging in /Users/zoe.slade/.local/share/virtualenvs/splink-bxsLLt4m/lib/python3.9/site-packages (from altair<6.0.0,>=5.0.1->splink==3.9.10) (23.0)\n",
      "Requirement already satisfied: toolz in /Users/zoe.slade/.local/share/virtualenvs/splink-bxsLLt4m/lib/python3.9/site-packages (from altair<6.0.0,>=5.0.1->splink==3.9.10) (0.12.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /Users/zoe.slade/.local/share/virtualenvs/splink-bxsLLt4m/lib/python3.9/site-packages (from altair<6.0.0,>=5.0.1->splink==3.9.10) (4.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/zoe.slade/.local/share/virtualenvs/splink-bxsLLt4m/lib/python3.9/site-packages (from Jinja2>=3.0.3->splink==3.9.10) (2.1.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/zoe.slade/.local/share/virtualenvs/splink-bxsLLt4m/lib/python3.9/site-packages (from jsonschema<5.0,>=3.2->splink==3.9.10) (22.2.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/zoe.slade/.local/share/virtualenvs/splink-bxsLLt4m/lib/python3.9/site-packages (from jsonschema<5.0,>=3.2->splink==3.9.10) (0.19.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/zoe.slade/.local/share/virtualenvs/splink-bxsLLt4m/lib/python3.9/site-packages (from pandas>1.3.0->splink==3.9.10) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/zoe.slade/.local/share/virtualenvs/splink-bxsLLt4m/lib/python3.9/site-packages (from pandas>1.3.0->splink==3.9.10) (2022.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/zoe.slade/.local/share/virtualenvs/splink-bxsLLt4m/lib/python3.9/site-packages (from pandas>1.3.0->splink==3.9.10) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/zoe.slade/.local/share/virtualenvs/splink-bxsLLt4m/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>1.3.0->splink==3.9.10) (1.16.0)\n",
      "Building wheels for collected packages: splink\n",
      "  Building editable for splink (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for splink: filename=splink-3.9.10-py3-none-any.whl size=6542 sha256=53134cc626ca8699f5bbcf731c4fc33477ade7bd6835fccd0ab29d25e86b5487\n",
      "  Stored in directory: /private/var/folders/nd/c3xr518x3txg5kcqp1h7zwc80000gp/T/pip-ephem-wheel-cache-di1qgpdv/wheels/88/cc/30/b622996cc540fe1fab8acfdbf9b4822b20095ff2d22050eba4\n",
      "Successfully built splink\n",
      "Installing collected packages: splink\n",
      "  Attempting uninstall: splink\n",
      "    Found existing installation: splink 3.9.10\n",
      "    Uninstalling splink-3.9.10:\n",
      "      Successfully uninstalled splink-3.9.10\n",
      "Successfully installed splink-3.9.10\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nd/c3xr518x3txg5kcqp1h7zwc80000gp/T/ipykernel_42678/4141705158.py:16: DeprecationWarning: `exact_match_rule` is deprecated; use `block_on`\n",
      "  exact_match_rule(\"surname\"),\n",
      "/var/folders/nd/c3xr518x3txg5kcqp1h7zwc80000gp/T/ipykernel_42678/4141705158.py:34: SplinkDeprecated: target_rows is deprecated; use max_pairs\n",
      "  linker.estimate_u_using_random_sampling(target_rows=1e6)\n",
      "----- Estimating u probabilities using random sampling -----\n",
      "\n",
      "Estimated u probabilities using random sampling\n",
      "\n",
      "Your model is not yet fully trained. Missing estimates for:\n",
      "    - first_name (no m values are trained).\n",
      "    - surname (no m values are trained).\n",
      "    - dob (no m values are trained).\n",
      "    - city (no m values are trained).\n",
      "    - email (no m values are trained).\n",
      "\n",
      "----- Starting EM training session -----\n",
      "\n",
      "Estimating the m probabilities of the model by blocking on:\n",
      "l.first_name = r.first_name and l.surname = r.surname\n",
      "\n",
      "Parameter estimates will be made for the following comparison(s):\n",
      "    - dob\n",
      "    - city\n",
      "    - email\n",
      "\n",
      "Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n",
      "    - first_name\n",
      "    - surname\n",
      "\n",
      "Iteration 1: Largest change in params was -0.501 in the m_probability of dob, level `Exact match`\n",
      "Iteration 2: Largest change in params was -0.0264 in the m_probability of city, level `Exact match`\n",
      "Iteration 3: Largest change in params was 0.0148 in probability_two_random_records_match\n",
      "Iteration 4: Largest change in params was 0.0109 in probability_two_random_records_match\n",
      "Iteration 5: Largest change in params was 0.00823 in probability_two_random_records_match\n",
      "\n",
      "EM converged after 5 iterations\n",
      "\n",
      "Your model is not yet fully trained. Missing estimates for:\n",
      "    - first_name (no m values are trained).\n",
      "    - surname (no m values are trained).\n",
      "\n",
      "----- Starting EM training session -----\n",
      "\n",
      "Estimating the m probabilities of the model by blocking on:\n",
      "l.dob = r.dob\n",
      "\n",
      "Parameter estimates will be made for the following comparison(s):\n",
      "    - first_name\n",
      "    - surname\n",
      "    - city\n",
      "    - email\n",
      "\n",
      "Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n",
      "    - dob\n",
      "\n",
      "Iteration 1: Largest change in params was 0.366 in the m_probability of surname, level `All other comparisons`\n",
      "Iteration 2: Largest change in params was 0.101 in probability_two_random_records_match\n",
      "Iteration 3: Largest change in params was 0.0421 in probability_two_random_records_match\n",
      "Iteration 4: Largest change in params was 0.0215 in probability_two_random_records_match\n",
      "Iteration 5: Largest change in params was 0.0127 in probability_two_random_records_match\n",
      "Iteration 6: Largest change in params was 0.0081 in probability_two_random_records_match\n",
      "\n",
      "EM converged after 6 iterations\n",
      "\n",
      "Your model is fully trained. All comparisons have at least one estimate for their m and u values\n",
      "Completed iteration 1, root rows count 0\n"
     ]
    }
   ],
   "source": [
    "from splink.datasets import splink_datasets\n",
    "from splink.duckdb.blocking_rule_library import block_on, exact_match_rule\n",
    "from splink.duckdb.comparison_library import (\n",
    "    exact_match,\n",
    "    levenshtein_at_thresholds,\n",
    ")\n",
    "from splink.duckdb.linker import DuckDBLinker\n",
    "\n",
    "df = splink_datasets.fake_1000\n",
    "\n",
    "settings = {\n",
    "    \"probability_two_random_records_match\": 0.01,\n",
    "    \"link_type\": \"dedupe_only\",\n",
    "    \"blocking_rules_to_generate_predictions\": [\n",
    "        block_on([\"first_name\"]),\n",
    "        exact_match_rule(\"surname\"),\n",
    "    ],\n",
    "    \"comparisons\": [\n",
    "        levenshtein_at_thresholds(\"first_name\", 2),\n",
    "        exact_match(\"surname\"),\n",
    "        exact_match(\"dob\"),\n",
    "        exact_match(\"city\", term_frequency_adjustments=True),\n",
    "        exact_match(\"email\"),\n",
    "    ],\n",
    "    \"retain_intermediate_calculation_columns\": True,\n",
    "    \"additional_columns_to_retain\": [\"cluster\"],\n",
    "    \"max_iterations\": 10,\n",
    "    \"em_convergence\": 0.01,\n",
    "}\n",
    "\n",
    "\n",
    "linker = DuckDBLinker(df, settings)\n",
    "\n",
    "linker.estimate_u_using_random_sampling(target_rows=1e6)\n",
    "\n",
    "\n",
    "blocking_rule = \"l.first_name = r.first_name and l.surname = r.surname\"\n",
    "linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n",
    "\n",
    "\n",
    "blocking_rule = \"l.dob = r.dob\"\n",
    "linker.estimate_parameters_using_expectation_maximisation(blocking_rule)\n",
    "\n",
    "\n",
    "df_predict = linker.predict()\n",
    "df_clustered = linker.cluster_pairwise_predictions_at_threshold(df_predict, 0.9)\n",
    "df_cluster_metrics = linker._compute_graph_metrics(df_predict, df_clustered, threshold_match_probability=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "linker.cluster_studio_dashboard(\n",
    "    df_predict,\n",
    "    df_clustered,\n",
    "    out_path=\"cluster_studio.html\",\n",
    "    sampling_method=\"lowest_density_clusters_by_size\",\n",
    "    sample_size =20,\n",
    "    overwrite=True,\n",
    "    _df_cluster_metrics=df_cluster_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Completed iteration 1, root rows count 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Table name in database: `__splink__cluster_metrics_clusters_5ba854f77`\n",
       "\n",
       "To retrieve records, you can call the following methods on this object:\n",
       "`.as_record_dict(limit=5)` or `.as_pandas_dataframe(limit=5)`.\n",
       "\n",
       "You may omit the `limit` argument to return all records.\n",
       "\n",
       "This table represents the following splink entity: __splink__cluster_metrics_clusters"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predict = linker.predict()\n",
    "df_clustered = linker.cluster_pairwise_predictions_at_threshold(df_predict, 0.9)\n",
    "df_cluster_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from splink.cluster_studio import _get_lowest_density_clusters\n",
    "from splink.duckdb.linker import DuckDBLinker\n",
    "\n",
    "\n",
    "def test_density_sample():\n",
    "    # Simple df and settings for linker\n",
    "    person_ids = [i + 1 for i in range(5)]\n",
    "    df = pd.DataFrame({\"person_id\": person_ids})\n",
    "\n",
    "    settings = {\n",
    "        \"link_type\": \"dedupe_only\",\n",
    "        \"unique_id_column_name\": \"person_id\",\n",
    "    }\n",
    "    linker = DuckDBLinker(df, settings)\n",
    "\n",
    "    # Dummy cluster metrics table\n",
    "    cluster = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\n",
    "    n_nodes = [2, 3, 3, 3, 10, 10]\n",
    "    n_edges = [1, 2, 2, 3, 9, 20]\n",
    "    density = [\n",
    "        (n_edges * 2) / (n_nodes * (n_nodes - 1))\n",
    "        for n_nodes, n_edges in zip(n_nodes, n_edges)\n",
    "    ]\n",
    "    pd_metrics = pd.DataFrame(\n",
    "        {\n",
    "            \"cluster_id\": cluster,\n",
    "            \"n_nodes\": n_nodes,\n",
    "            \"n_edges\": n_edges,\n",
    "            \"density\": density,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Convert to Splink dataframe\n",
    "    df_cluster_metrics = linker.register_table(\n",
    "        pd_metrics, \"df_cluster_metrics\", overwrite=True\n",
    "    )\n",
    "    result = _get_lowest_density_clusters(\n",
    "        linker, df_cluster_metrics, rows_per_partition=1, min_nodes=3\n",
    "    )\n",
    "\n",
    "    result = sorted(result, key=lambda x: x[\"cluster_id\"])\n",
    "\n",
    "    expect = [\n",
    "        {\"cluster_id\": \"C\", \"density_4dp\": 0.6667},\n",
    "        {\"cluster_id\": \"E\", \"density_4dp\": 0.2},\n",
    "    ]\n",
    "\n",
    "    # assert result == expect\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cluster_id': 'B', 'density_4dp': 0.6667},\n",
       " {'cluster_id': 'E', 'density_4dp': 0.2}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_density_sample()\n",
    "# type(result)\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on historical 50k - working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_hist = pd.read_csv(\"clusters_hist_50k.csv\")\n",
    "edges_hist = pd.read_csv(\"edges_hist_50k.csv\")\n",
    "\n",
    "# Update linker unique id to match data\n",
    "linker._settings_obj._unique_id_column_name = \"unique_id\"\n",
    "\n",
    "# Convert to splink dataframes\n",
    "df_predict = linker.register_table(edges_hist, \"df_predict\", overwrite=True)\n",
    "df_clustered = linker.register_table(clusters_hist, \"df_clustered\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate density metrics\n",
    "\n",
    "df_cluster_metrics = linker._compute_cluster_metrics(df_predict, df_clustered, 0.99)\n",
    "df_cluster_metrics.as_pandas_dataframe().sort_values(by='density').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linker.cluster_studio_dashboard(\n",
    "    df_predict,\n",
    "    df_clustered,\n",
    "    out_path=\"cluster_studio.html\",\n",
    "    sampling_method=\"lowest_density_clusters\",\n",
    "    sample_size=10,\n",
    "    overwrite=True,\n",
    "    _df_cluster_metrics=df_cluster_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out density produced with modified version of function\n",
    "\n",
    "def _get_cluster_id_by_density(\n",
    "    linker, df_cluster_metrics, sample_size: int, min_nodes: int\n",
    "):\n",
    "    # Ordering: least dense clusters first\n",
    "    sql = f\"\"\"\n",
    "    SELECT cluster_id, density, n_nodes, n_edges, \n",
    "    (n_edges * 2)/(n_nodes * (n_nodes-1)) AS density_check\n",
    "    FROM {df_cluster_metrics.physical_name}\n",
    "    WHERE n_nodes >= {min_nodes}\n",
    "    ORDER BY density\n",
    "    LIMIT {sample_size}\n",
    "    \"\"\"\n",
    "\n",
    "    df_density_sample = linker._sql_to_splink_dataframe_checking_cache(\n",
    "        sql, \"__splink__density_sample\"\n",
    "    )\n",
    "\n",
    "    return df_density_sample.as_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_get_cluster_id_by_density(linker, df_cluster_metrics, sample_size=10, min_nodes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "- Correct (lowest density) clusters being found and put into splink cluster studio\n",
    "- Ordering of clusters isn't from low to high density - density info is lost so might be easier to add the density to drop down menu\n",
    "- Density being calculated correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Nomis data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem is that have two nodes in clusters table for 6 edges legit edges with threshold above 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splink.duckdb.linker import DuckDBLinker\n",
    "import pandas as pd\n",
    "\n",
    "# Set up linker\n",
    "\n",
    "# Simple dummy df\n",
    "person_ids = [i + 1 for i in range(5)]\n",
    "df = pd.DataFrame({\"person_id\": person_ids})\n",
    "\n",
    "settings = {\n",
    "    \"link_type\": \"dedupe_only\",\n",
    "    \"unique_id_column_name\": \"person_id\",\n",
    "    \"retain_intermediate_calculation_columns\": True,\n",
    "}\n",
    "linker = DuckDBLinker(df, settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up edges and clusters data\n",
    "\n",
    "edges_nomis = pd.read_csv(\"nomis_edges_anonymised.csv\")\n",
    "# edges_nomis[\"person_id_l\"] = edges_nomis[\"person_id_l\"].astype(int)\n",
    "clusters_nomis = pd.read_csv(\"nomis_clusters_anonymised.csv\")\n",
    "\n",
    "# Give cols conventional names\n",
    "# Change cluster_low to cluster_x for threshold x\n",
    "clusters_nomis = clusters_nomis.rename(columns={\"cluster_low\": \"cluster_id\"})\n",
    "\n",
    "# Transform to Splink dataframes\n",
    "df_edges_nomis = linker.register_table(edges_nomis, \"edges_nomis\", overwrite=True)\n",
    "df_clusters_nomis = linker.register_table(\n",
    "    clusters_nomis, \"clusters_nomis\", overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate density metrics\n",
    "\n",
    "# linker.debug_mode=True\n",
    "\n",
    "df_cluster_metrics_nomis = linker._compute_cluster_metrics(\n",
    "    df_edges_nomis, df_clusters_nomis, threshold_match_probability=0.95\n",
    ")\n",
    "# df_cluster_metrics_nomis.as_pandas_dataframe()\n",
    "\n",
    "# df_cluster_metrics_nomis.as_pandas_dataframe().groupby(\"n_nodes\").min(\"density\").head()\n",
    "\n",
    "df_cluster_metrics_nomis.as_pandas_dataframe().groupby(\"density\").min('density').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = 10\n",
    "n_edges = 30\n",
    "\n",
    "density = (n_edges * 2) / (n_nodes * (n_nodes - 1))\n",
    "density\n",
    "\n",
    "# Density calculated correctly\n",
    "# Thing that is wrong is having 6 edges when only 2 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster id = d79b5dfd903fb222e662b0eb96ccfc73\n",
    "Appears twice in the clusters table - so already not enough nodes for the number of edges\n",
    "\n",
    "dcdcba59f8e31e4bfebc6aa1e99e3f1f = person id, occurs 5 times in edges left at or above 0.95 threshold\n",
    "\n",
    "9be9a90df36aad592ea1e88b136859b3 = person id, occurs 1 time in edge left at or above the threshold. This is to be expected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try generating the clusters data again from nomis edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linker.debug_mode = False\n",
    "\n",
    "nomis_predict = pd.read_csv(\"nomis_edges_anonymised.csv\")\n",
    "# nomis_predict[\"person_id_l\"] = nomis_predict[\"person_id_l\"].astype(int)\n",
    "# nomis_predict[\"person_id_r\"] = nomis_predict[\"person_id_r\"].astype(int)\n",
    "\n",
    "\n",
    "# Transform to Splink dataframes\n",
    "df_nomis_predict = linker.register_table(nomis_predict, \"nomis_predict\", overwrite=True)\n",
    "\n",
    "new_nomis_clusters = linker.cluster_pairwise_predictions_at_threshold(\n",
    "    df_nomis_predict, 0.9\n",
    ")\n",
    "display(new_nomis_clusters.as_pandas_dataframe().sort_values(\"cluster_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linker.cluster_studio_dashboard(\n",
    "    df_edges_nomis,\n",
    "    df_clusters_nomis,\n",
    "    out_path=\"cluster_studio.html\",\n",
    "    sampling_method=\"by_cluster_density\",\n",
    "    sample_size=10,\n",
    "    overwrite=True,\n",
    "    _df_cluster_metrics=df_cluster_metrics_nomis,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building actual test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from splink.cluster_studio import _get_cluster_id_by_density\n",
    "\n",
    "from splink.duckdb.linker import DuckDBLinker\n",
    "\n",
    "# Dummy df and settings for linker\n",
    "person_ids = [i + 1 for i in range(5)]\n",
    "df = pd.DataFrame({\"person_id\": person_ids})\n",
    "\n",
    "settings = {\n",
    "    \"link_type\": \"dedupe_only\",\n",
    "    \"unique_id_column_name\": \"person_id\",\n",
    "}\n",
    "linker = DuckDBLinker(df, settings)\n",
    "\n",
    "# Dummy cluster metrics table\n",
    "cluster = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "n_nodes = [3, 2, 10, 3, 19]\n",
    "n_edges = [2, 1, 5, 2, 25]\n",
    "density = [\n",
    "    (n_edges * 2) / (n_nodes * (n_nodes - 1))\n",
    "    for n_nodes, n_edges in zip(n_nodes, n_edges)\n",
    "]\n",
    "df_metrics = pd.DataFrame(\n",
    "    {\"cluster_id\": cluster, \"n_nodes\": n_nodes, \"n_edges\": n_edges, \"density\": density}\n",
    ")\n",
    "df_metrics\n",
    "\n",
    "# Convert to Splink dataframe\n",
    "df_cluster_metrics = linker.register_table(\n",
    "    df_metrics, \"df_cluster_metrics\", overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing this function\n",
    "\n",
    "def _get_cluster_id_by_density(\n",
    "    linker, df_cluster_metrics, sample_size: int, min_nodes: int\n",
    "):\n",
    "    # Ordering: least dense clusters first\n",
    "    sql = f\"\"\"\n",
    "    SELECT cluster_id\n",
    "    FROM {df_cluster_metrics.physical_name}\n",
    "    WHERE n_nodes >= {min_nodes}\n",
    "    ORDER BY density\n",
    "    LIMIT {sample_size}\n",
    "    \"\"\"\n",
    "\n",
    "    df_density_sample = linker._sql_to_splink_dataframe_checking_cache(\n",
    "        sql, \"__splink__density_sample\"\n",
    "    )\n",
    "\n",
    "    return [r[\"cluster_id\"] for r in df_density_sample.as_record_dict()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = _get_cluster_id_by_density(linker, df_cluster_metrics, sample_size=3, min_nodes=3)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better to put the linker inside the function?\n",
    "\n",
    "def test_density_sample():\n",
    "    df_result = _get_cluster_id_by_density(\n",
    "        linker, df_cluster_metrics, sample_size=3, min_nodes=3\n",
    "    )\n",
    "    df_expect = [\"C\", \"E\", \"A\"]\n",
    "    assert df_result == df_expect\n",
    "\n",
    "test_density_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_size_density():\n",
    "    # Linker with basic settings\n",
    "    settings = {\"link_type\": \"dedupe_only\", \"unique_id_column_name\": \"person_id\"}\n",
    "    linker = DuckDBLinker(df, settings)\n",
    "\n",
    "    # Register as Splink dataframes\n",
    "    df_predict = linker.register_table(edges, \"df_predict\", overwrite=True)\n",
    "    df_clustered = linker.register_table(clusters, \"df_clustered\", overwrite=True)\n",
    "\n",
    "    df_cluster_metrics = linker._compute_cluster_metrics(\n",
    "        df_predict, df_clustered, threshold_match_probability=0.99\n",
    "    )\n",
    "    df_cluster_metrics = df_cluster_metrics.as_pandas_dataframe()\n",
    "\n",
    "    assert_frame_equal(df_cluster_metrics, df_expected)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "splink-bxsLLt4m",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
