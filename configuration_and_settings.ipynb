{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customising splink - configuration and settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In the [Quickstart demo](\"quickstart_demo.ipynb\") we saw an example of how to use `splink`.  There was minimal customisation of the settings - the demo relies on the fact that when settings are not specified by the user, `splink` uses sensible default values.  \n",
    "\n",
    "Not all settings have defaults - at minimum the user needs to choose the `link_type`, the `blocking_rules` and the `comparison_columns`.\n",
    "\n",
    "In most real-world applications, more accurate results will be obtained by customising the settings, often by trial and error. \n",
    "\n",
    "The main way to do this is through the `settings` dictionary.  This is passed to `splink` like so:\n",
    "\n",
    "\n",
    "```\n",
    "from splink import Splink\n",
    "\n",
    "settings = { }  # Settings dictionary goes here\n",
    "\n",
    "linker = Splink(settings, spark, df=df)\n",
    "```\n",
    "\n",
    "This settings dictionary can be quite complicated, so this notebook provides details of the various settings and what they do.  \n",
    "\n",
    "We recommend using it alongside our [autocompleting settings editor](https://robinlinacre.com/simple_sparklink_settings_editor/), which makes it quicker and easier to write settings dictionaries.  This validates the your settings against the [json schema](https://github.com/moj-analytical-services/sparklink/blob/dev/sparklink/files/settings_jsonschema.json) for the settings.  Note: you can use this schema in some text editors to enable autocompletion - e.g. see [here](https://code.visualstudio.com/docs/languages/json#_intellisense-and-validation) for VS Code.\n",
    "\n",
    "You can also validate a settings object within Python with the following code:\n",
    "\n",
    "```\n",
    "from sparklink.validate import validate_settings\n",
    "validate_settings(settings)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the link type\n",
    "\n",
    "There are three types of data linking or deduplication built into `splink`, which are configured by setting the `link_type` key of the settings dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Summary**:\n",
       "The type of data linking task - `dedupe_only`, `link_only` or `link_and_dedupe`.  Required.\n",
       "\n",
       "**Description**:\n",
       "- When `dedupe_only`, the user provides a single input dataframe, and `splink` tries to find duplicate entries\n",
       "- When `link_only`, the user provides two dataframes, and `splink` tries to find a link between two two.  It makes no attempt to deduplicate datasets so this is best used when input datasets contain no duplicates\n",
       "- When `link_and_dedupe`, the user provides two dataframes, and `splink` simultanouesly links and dedupes the dataframes.\n",
       "\n",
       "**Data type**: string\n",
       "\n",
       "**Possible values**: `dedupe_only`, `link_only`, `link_and_dedupe`\n",
       "\n",
       "**Example**:\n",
       "\n",
       "```\n",
       "settings = {\n",
       "    \"link_type\": \"dedupe_only\"\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from demo_notebooks.demo_utils import render_key_as_markdown\n",
    "render_key_as_markdown(\"link_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing your blocking rules (or cartesian join)\n",
    "\n",
    "In most linking tasks, you will need to choose one or more [blocking rules](https://www.isi.edu/integration/papers/michelson06-aaai.pdf), which are used as a pre-processing step to eliminate implausible matches.  Without blocking, Apache Spark will compare all records to one another, which is usually computationally intractable (for linking problems of over about 10,000-50,000 records).  \n",
    "\n",
    "These are specified using the `blocking_rules` key of the `settings` dictionary.\n",
    "\n",
    "Alternatively, if you really do want to compare all records to one another, you can set the `cartesian_join` key instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocking rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Summary**:\n",
       "A list of one or more blocking rules to apply. Ignored if cartesian_product=true is set\n",
       "\n",
       "**Description**:\n",
       "Each rule is a SQL expression representing the blocking rule, which will be used to create a join.  The left table is aliased with `l` and the right table is aliased with `r`. For example, if you want to block on a `first_name` column, the blocking rule would be `l.first_name = r.first_name`\n",
       "\n",
       "**Data type**: array\n",
       "\n",
       "**Example**:\n",
       "\n",
       "```\n",
       "settings = {\n",
       "    \"blocking_rules\": ['l.first_name = r.first_name AND l.surname = r.surname', 'l.dob = r.dob']\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from demo_notebooks.demo_utils import render_key_as_markdown\n",
    "render_key_as_markdown(\"blocking_rules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cartesian join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Summary**:\n",
       "If set to true, all comparisons between the input datasets will be generated\n",
       "\n",
       "**Description**:\n",
       "For large input datasets, the will generally be computationally intractable because it will generate numrows squared comparisons\n",
       "\n",
       "**Data type**: boolean\n",
       "\n",
       "**Example**:\n",
       "\n",
       "```\n",
       "settings = {\n",
       "    \"cartesian_product\": False\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "render_key_as_markdown(\"cartesian_product\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing which columns should be used to link data\n",
    "\n",
    "The user must decide which columns in the input datasets will be used to link data.  For example, if the user is deduplicating a table of people, it would be typical to use personal identifiers like name and date of birth.\n",
    "\n",
    "These columns, and configuration options for each individual column, are provided as a list of dictionaries assigned to the `column_settings` key of the `settings` dictionary.\n",
    "\n",
    "For example:\n",
    "```\n",
    "settings = {\n",
    "    \"column_settings\": [\n",
    "    {\n",
    "        \"column_name\": \"first_name\"\n",
    "    },\n",
    "    {\n",
    "        \"column_name\": \"latitude\",\n",
    "        \"data_type\": \"numeric\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "**Note that, throughout the splink package, it is assumed that incoming datasets have been prepared so they have common column names - e.g. if you are linking two datasets on first name, the column containing this data has the same name in both datasets.  This cannot be configured and the package will not work without this preparation step.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"- When `dedupe_only`, the user provides a single input dataframe, and `splink` tries to find duplicate entries\\n- When `link_only`, the user provides two dataframes, and `splink` tries to find a link between two two.  It makes no attempt to deduplicate datasets so this is best used when input datasets contain no duplicates\\n- When `link_and_dedupe`, the user provides two dataframes, and `splink` simultanouesly links and dedupes the dataframes.\"\n"
     ]
    }
   ],
   "source": [
    "s = \"\"\"- When `dedupe_only`, the user provides a single input dataframe, and `splink` tries to find duplicate entries\n",
    "- When `link_only`, the user provides two dataframes, and `splink` tries to find a link between two two.  It makes no attempt to deduplicate datasets so this is best used when input datasets contain no duplicates\n",
    "- When `link_and_dedupe`, the user provides two dataframes, and `splink` simultanouesly links and dedupes the dataframes.\"\"\"\n",
    "import json\n",
    "print(json.dumps(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
